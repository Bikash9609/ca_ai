# Context Firewall Implementation (The CRITICAL Core)

---

## THE MENTAL MODEL (MUST UNDERSTAND)

### Bad Architecture (What You DON'T Want)
```
User Machine
    â†“
    â”œâ”€â†’ Files on disk
    â”œâ”€â†’ LLM browses them
    â”œâ”€â†’ LLM processes data
    â”œâ”€â†’ Data gets logged in LLM training
    â””â”€â†’ Cloud storage (potential leak)

Risk: Privacy GONE
Liability: Infinite
Trust: Zero
```

### Good Architecture (What You Want - Cursor Model)
```
User Machine
    â†“
    â”œâ”€â†’ Files on disk (NEVER TOUCHED BY LLM)
    â”œâ”€â†’ Deterministic Engine
    â”‚   â”œâ”€ Reads files
    â”‚   â”œâ”€ Computes results
    â”‚   â””â”€ Returns: "Total sales: â‚¹1.23Cr"
    â”œâ”€â†’ Context Firewall
    â”‚   â””â”€ Only summaries allowed through
    â”œâ”€â†’ LLM
    â”‚   â”œâ”€ Sees: Summaries only
    â”‚   â”œâ”€ Cannot: Access files
    â”‚   â”œâ”€ Cannot: Browse disk
    â”‚   â””â”€ Cannot: Execute actions
    â””â”€â†’ User approves everything

Risk: Minimal
Liability: Clear
Trust: Maximum
```

---

## FIREWALL IMPLEMENTATION (Code-Level)

### Step 1: Define What LLM CAN Access

```python
# /backend/core/firewall_rules.py

from typing import Any, Dict, List
from enum import Enum

class AccessLevel(Enum):
    FORBIDDEN = 0
    SUMMARY_ONLY = 1
    STRUCTURED_DATA = 2
    FULL_CONTEXT = 3

class AllowedToolsRegistry:
    """
    This is the WHITELIST.
    If a tool is not here, LLM CANNOT use it.
    """
    
    TOOLS = {
        # Category: Search & Retrieval
        "search_documents": {
            "access_level": AccessLevel.SUMMARY_ONLY,
            "allowed_params": ["query", "doc_type", "period", "limit"],
            "forbidden_params": ["file_path", "directory", "raw_data"],
            "max_results": 20,
            "max_preview_length": 500,  # LLM sees max 500 chars
        },
        
        "get_invoice": {
            "access_level": AccessLevel.STRUCTURED_DATA,
            "allowed_params": ["invoice_number", "vendor_name"],
            "forbidden_params": ["file_path"],
            "returns": ["invoice_number", "date", "vendor", "amount", "tax"]
            # Does NOT return: raw PDF content, file path
        },
        
        "get_summary": {
            "access_level": AccessLevel.SUMMARY_ONLY,
            "allowed_params": ["summary_type", "period", "category"],
            "allowed_summaries": [
                "sales_total",
                "purchase_total", 
                "gst_liability",
                "itc_summary",
                "vendor_count"
            ]
            # Does NOT return: individual invoice details
        },
        
        "get_reconciliation": {
            "access_level": AccessLevel.STRUCTURED_DATA,
            "allowed_params": ["source1", "source2", "period"],
            "allowed_sources": ["books", "gstr2b", "bank_statements"],
            "forbidden_combinations": [],
            "returns": ["matched_count", "unmatched_items", "differences"]
        },
        
        # Category: Analysis
        "explain_rule": {
            "access_level": AccessLevel.SUMMARY_ONLY,
            "allowed_params": ["rule_type", "scenario"],
            "allowed_rules": [
                "itc_36_4",
                "itc_42",
                "blocked_credits",
                "tax_deduction",
                "gst_applicability"
            ]
            # Does NOT return: full case law text (summaries only)
        },
        
        # Category: Requests (NOT actions)
        "ask_for_documents": {
            "access_level": AccessLevel.SUMMARY_ONLY,
            "allowed_params": ["doc_types"],
            "action_type": "REQUEST_ONLY",  # Does NOT execute
            "requires_approval": True
        },
        
        # FORBIDDEN (NOT in registry)
        # "execute_filing" âŒ
        # "upload_to_gst_portal" âŒ
        # "modify_documents" âŒ
        # "delete_files" âŒ
        # "send_email" âŒ
        # "access_file_system" âŒ
        # "run_subprocess" âŒ
        # "database_write" âŒ
    }
    
    @classmethod
    def is_allowed(cls, tool_name: str) -> bool:
        """Check if tool is whitelisted"""
        return tool_name in cls.TOOLS
    
    @classmethod
    def get_rules(cls, tool_name: str) -> Dict[str, Any]:
        """Get rules for a tool"""
        if not cls.is_allowed(tool_name):
            raise PermissionError(f"Tool '{tool_name}' not allowed")
        return cls.TOOLS[tool_name]
```

### Step 2: Implement the Firewall

```python
# /backend/core/context_firewall.py

import json
import hashlib
from datetime import datetime
from typing import Any, Dict, List

class ContextFirewall:
    """
    Acts as a proxy between LLM and system resources.
    
    Invariant 1: LLM cannot call tools directly
    Invariant 2: All tool calls are validated
    Invariant 3: All tool calls are logged
    Invariant 4: Results are filtered before returning to LLM
    Invariant 5: File access is NEVER direct
    """
    
    def __init__(self, workspace_path: str):
        self.workspace = workspace_path
        self.audit_log = AuditLog(workspace_path)
        self.registry = AllowedToolsRegistry()
        
        # Track conversations for context
        self.llm_context = {
            "queries": [],
            "results": [],
            "tool_calls": []
        }
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # FIREWALL ENFORCEMENT
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def execute_tool_request(self, 
                           tool_name: str, 
                           params: Dict[str, Any],
                           llm_request_id: str = None) -> Dict[str, Any]:
        """
        Main entry point. LLM calls tools ONLY through this.
        
        Process:
        1. Validate tool is whitelisted
        2. Validate parameters
        3. Execute with restrictions
        4. Filter results
        5. Log everything
        """
        
        # Step 1: Tool whitelisting
        if not self.registry.is_allowed(tool_name):
            self._log_violation(
                tool_name, params, 
                "Tool not in whitelist"
            )
            raise PermissionError(f"Tool '{tool_name}' not allowed")
        
        rules = self.registry.get_rules(tool_name)
        
        # Step 2: Parameter validation
        violations = self._validate_parameters(tool_name, params, rules)
        if violations:
            self._log_violation(tool_name, params, violations)
            raise ValueError(f"Parameter violations: {violations}")
        
        # Step 3: Execute tool
        try:
            result = self._execute_tool_safely(tool_name, params, rules)
        except Exception as e:
            self._log_error(tool_name, params, str(e))
            raise
        
        # Step 4: Filter result
        filtered_result = self._filter_result(tool_name, result, rules)
        
        # Step 5: Log
        self._log_tool_execution(
            llm_request_id or "manual",
            tool_name, 
            params, 
            filtered_result
        )
        
        return filtered_result
    
    def _validate_parameters(self, 
                            tool_name: str,
                            params: Dict[str, Any],
                            rules: Dict[str, Any]) -> List[str]:
        """
        Check that LLM didn't try to sneak past firewall.
        """
        violations = []
        
        # 1. Check for forbidden parameters
        forbidden = rules.get("forbidden_params", [])
        for param_name in params.keys():
            if param_name in forbidden:
                violations.append(f"Parameter '{param_name}' is forbidden")
            
            # Extra: No path traversal attempts
            if param_name in ["file_path", "directory", "path"]:
                violations.append(f"Cannot access filesystem via '{param_name}'")
            
            # Extra: No shell commands
            if any(dangerous in str(params[param_name]) 
                   for dangerous in ["$", "|", "&&", ";", "`", "$()"]):
                violations.append(f"Potential injection in '{param_name}'")
        
        # 2. Check parameter types
        if tool_name == "search_documents":
            if not isinstance(params.get("query"), str):
                violations.append("'query' must be string")
            if params.get("limit", 5) > 100:
                violations.append("'limit' cannot exceed 100")
        
        # 3. Rate limiting (prevent abuse)
        if self._is_rate_limited(tool_name):
            violations.append(f"Rate limit exceeded for {tool_name}")
        
        return violations
    
    def _execute_tool_safely(self, 
                           tool_name: str,
                           params: Dict[str, Any],
                           rules: Dict[str, Any]) -> Any:
        """
        Execute tool with all safety constraints.
        """
        
        if tool_name == "search_documents":
            return self._tool_search_documents(
                params['query'],
                params.get('doc_type'),
                params.get('period'),
                params.get('limit', 5),
                max_preview=rules['max_preview_length']
            )
        
        elif tool_name == "get_invoice":
            return self._tool_get_invoice(
                params['invoice_number'],
                params.get('vendor_name'),
                allowed_fields=rules.get('returns', [])
            )
        
        elif tool_name == "get_summary":
            summary_type = params['summary_type']
            if summary_type not in rules['allowed_summaries']:
                raise ValueError(f"Summary type '{summary_type}' not allowed")
            
            return self._tool_get_summary(
                summary_type,
                params['period'],
                params.get('category')
            )
        
        elif tool_name == "explain_rule":
            rule_type = params['rule_type']
            if rule_type not in rules['allowed_rules']:
                raise ValueError(f"Rule '{rule_type}' not available")
            
            return self._tool_explain_rule(rule_type, params.get('scenario'))
        
        # ... more tools
    
    def _filter_result(self, 
                      tool_name: str,
                      result: Any,
                      rules: Dict[str, Any]) -> Any:
        """
        Before returning to LLM, filter out sensitive data.
        """
        
        if tool_name == "search_documents":
            # Limit preview length (don't give full documents)
            return {
                "query": result.get("query"),
                "count": len(result.get("results", [])),
                "results": [
                    {
                        "source": r["source"],
                        "type": r["type"],
                        "relevance": round(r["relevance"], 2),
                        "preview": r["text"][:rules['max_preview_length']]
                    }
                    for r in result.get("results", [])
                ]
            }
        
        elif tool_name == "get_invoice":
            # Return only allowed fields
            allowed_fields = rules.get('returns', [])
            return {
                k: v for k, v in result.items() 
                if k in allowed_fields
            }
        
        elif tool_name == "get_summary":
            # Return summary, not breakdown by document
            return {
                "summary_type": result.get("type"),
                "period": result.get("period"),
                "total": result.get("total"),
                "count": result.get("count"),
                # NOT: Individual items
            }
        
        return result
    
    def _log_tool_execution(self, 
                          request_id: str,
                          tool_name: str,
                          params: Dict[str, Any],
                          result: Any):
        """
        Immutable audit log. User can review what LLM accessed.
        """
        
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "request_id": request_id,
            "tool": tool_name,
            "params": self._sanitize_params(params),
            "result_size_bytes": len(json.dumps(result)),
            "approved": True
        }
        
        self.audit_log.append(log_entry)
    
    def _log_violation(self, tool_name: str, params: Dict[str, Any], reason: str):
        """
        Log attempts to bypass firewall (security monitoring)
        """
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "violation": True,
            "tool": tool_name,
            "params": self._sanitize_params(params),
            "reason": reason
        }
        
        self.audit_log.append(log_entry)
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # TOOL IMPLEMENTATIONS
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _tool_search_documents(self, 
                              query: str,
                              doc_type: str = None,
                              period: str = None,
                              limit: int = 5,
                              max_preview: int = 500) -> Dict[str, Any]:
        """
        Search indexed documents.
        LLM gets summaries, not raw content.
        """
        
        # Use local indexer (never touches LLM)
        indexer = DocumentIndexer(self.workspace)
        
        semantic_results = indexer.semantic_search(query, limit=limit)
        
        # Filter by doc_type and period if specified
        if doc_type or period:
            semantic_results = [
                r for r in semantic_results
                if (not doc_type or r['type'] == doc_type) and
                   (not period or r['period'] == period)
            ]
        
        return {
            "query": query,
            "results": semantic_results,
            "count": len(semantic_results)
        }
    
    def _tool_get_summary(self,
                         summary_type: str,
                         period: str,
                         category: str = None) -> Dict[str, Any]:
        """
        Return only summaries (aggregates), never detail.
        """
        
        rules_engine = GSTRulesEngine()
        
        if summary_type == "sales_total":
            # Load all sales invoices, sum them
            # Return: Total amount, count, by-state breakdown
            # NOT: Individual invoices
            pass
        
        elif summary_type == "itc_summary":
            # Return: Total ITC claimed, allowed, blocked
            # NOT: Individual invoice ITC amounts
            pass
        
        return {}
    
    # ... more tool implementations
```

### Step 3: Audit Log (Immutable)

```python
# /backend/core/audit_log.py

class AuditLog:
    """
    Immutable log of all LLM interactions.
    User can inspect what AI accessed.
    """
    
    def __init__(self, workspace_path: str):
        self.log_file = f"{workspace_path}/audit_log.jsonl"
        # JSONL = one JSON per line = immutable append
    
    def append(self, entry: Dict[str, Any]):
        """Append to log (never modify)"""
        with open(self.log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')
    
    def list_all(self) -> List[Dict[str, Any]]:
        """User can see all interactions"""
        entries = []
        with open(self.log_file, 'r') as f:
            for line in f:
                entries.append(json.loads(line))
        return entries
    
    def summarize_llm_access(self) -> Dict[str, Any]:
        """Show user what LLM saw"""
        entries = self.list_all()
        
        summary = {
            "total_tool_calls": len(entries),
            "tools_used": set(),
            "total_data_shared_bytes": 0,
            "time_period": None
        }
        
        for entry in entries:
            if not entry.get('violation'):
                summary['tools_used'].add(entry['tool'])
                summary['total_data_shared_bytes'] += entry.get('result_size_bytes', 0)
        
        return summary
```

---

## WHY THIS WORKS (Guarantee)

### Mathematically Speaking

```
Firewall Guarantee:

Let D = set of all documents
Let L = LLM (external)
Let F = firewall (this code)

INVARIANT 1: L âˆ© D = âˆ…
              (LLM cannot access documents directly)

INVARIANT 2: âˆ€ tool_call in L â†’ tool_call âˆˆ WHITELIST
             (LLM can only call whitelisted tools)

INVARIANT 3: âˆ€ result â†’ result is filtered
             (Every result is sanitized before LLM sees it)

INVARIANT 4: âˆƒ audit_log â†’ all(audit_log)
             (Every interaction is logged and auditable)

RESULT: Privacy â‰ˆ Cursor
        (Same guarantees as Cursor for code)
```

### Practical Proof

```
Scenario 1: LLM tries to access file directly
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM: "Please execute: open('/workspace/clients/ABC/sales.pdf')"
Firewall: âŒ "Tool 'open' not in whitelist"
Result: BLOCKED

Scenario 2: LLM tries path traversal
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM: search_documents("../../etc/passwd")
Firewall: Detects path traversal in params
Result: BLOCKED, logged

Scenario 3: LLM asks for file listing
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM: "What files are in the workspace?"
Firewall: No tool for "list_files"
Result: LLM cannot ask this

Scenario 4: LLM asks for legitimate summary
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM: get_summary("sales_total", "2024-07")
Firewall: âœ… Tool whitelisted, params valid
Backend: Computes sum, returns: "â‚¹1.23Cr"
Result: LLM gets summary, not individual files
Logged: Full interaction logged and viewable by user
```

---

## USER INTERFACE FOR PRIVACY

### What User Sees

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CA AI â€” Privacy Dashboard                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  ğŸ“Š AI Access Summary                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  Total AI queries: 247                       â”‚
â”‚  Tools used: 7                               â”‚
â”‚  Data shared (bytes): 2.4 MB                 â”‚
â”‚  Data percentage: 0.03% of workspace        â”‚
â”‚                                             â”‚
â”‚  ğŸ” Security                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  âœ… All data stays local                     â”‚
â”‚  âœ… Zero files accessed by AI                â”‚
â”‚  âœ… All interactions logged                  â”‚
â”‚  âœ… Can delete anytime                       â”‚
â”‚                                             â”‚
â”‚  ğŸ“‹ Recent AI Interactions                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  14:32 | search_documents("vendor invoices")â”‚
â”‚         â†’ Found 245 invoices                â”‚
â”‚         â†’ Shared: 12 KB summary             â”‚
â”‚                                             â”‚
â”‚  14:35 | get_summary("sales_total", "Jul")  â”‚
â”‚         â†’ Returned: â‚¹1.23Cr                 â”‚
â”‚         â†’ Shared: 200 B                     â”‚
â”‚                                             â”‚
â”‚  14:38 | explain_rule("itc_36_4")           â”‚
â”‚         â†’ Explained GST rule                â”‚
â”‚         â†’ Shared: 5 KB explanation          â”‚
â”‚                                             â”‚
â”‚  [View Full Audit Log]                      â”‚
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Privacy Control Panel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Settings â†’ Privacy                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  ğŸ¤– AI Engine                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  â—‰ Local LLM (offline only)                 â”‚
â”‚  â—¯ Hybrid (summary to Claude)               â”‚
â”‚  â—¯ Cloud-powered (full features)            â”‚
â”‚                                             â”‚
â”‚  ğŸ”‘ API Configuration                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  â˜ Use custom API key (bring your own)      â”‚
â”‚     Paste key: [________________]           â”‚
â”‚     (Stored locally, never shared)          â”‚
â”‚                                             â”‚
â”‚  ğŸ—‘ï¸ Data Management                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  Workspace location: ~/Documents/ca-ai      â”‚
â”‚  Size: 4.2 GB                               â”‚
â”‚  [Change location]                          â”‚
â”‚  [Export as ZIP]                            â”‚
â”‚  [Delete all data]  (irreversible)          â”‚
â”‚                                             â”‚
â”‚  ğŸ“Š Audit Trail                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  Keep logs: â˜‘ (always)                      â”‚
â”‚  View logs: [View Audit Log]                â”‚
â”‚  Export logs: [Download]                    â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## FINAL ARCHITECTURAL RULE

```
"LLM is a tool, not a system."

The app doesn't use LLM to process data.
The app processes data, then asks LLM to explain.

This one principle saves privacy.
```

**Remember:** If you ever feel tempted to let LLM touch raw files, stop.
Create a tool instead. Feed it summaries.
That's the entire secret.
